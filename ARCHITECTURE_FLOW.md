# ğŸ§  Memory System Architecture â€” Complete Flow Explanation

## ğŸ“Š Three Storage Layers

### 1ï¸âƒ£ **Short-term Memory (Session Memory)**
- **Storage**: Python dictionary in RAM (`sessions[user_id]`)
- **Capacity**: Last 10-20 turns
- **Lifecycle**: Ephemeral (lost on restart)
- **Purpose**: Immediate conversational context

```python
# In app.py
sessions: dict[str, SessionMemory] = {}  # user_id -> SessionMemory object

# SessionMemory holds:
# - history: list of {"role": "user"|"assistant", "content": "..."}
# - Sliding window: keeps only last N messages
```

---

### 2ï¸âƒ£ **Mid-term Memory (Episodic Memory)**
- **Storage**: PostgreSQL table `episodic_memory` with vector embeddings
- **Capacity**: Unlimited (grows with conversation length)
- **Lifecycle**: Persistent (survives restart)
- **Purpose**: Compressed conversation history for semantic retrieval

```sql
-- Database schema
CREATE TABLE episodic_memory (
    id              SERIAL PRIMARY KEY,
    user_id         TEXT NOT NULL,
    turn_range      TEXT,           -- "turns 21-40"
    summary         TEXT NOT NULL,   -- Compressed conversation
    embedding       vector(384),     -- For similarity search
    turn_start      INT,
    turn_end        INT,
    created_at      TIMESTAMPTZ
);
```

**Key Properties:**
- Contains **summaries** (not raw turns)
- Has **vector embeddings** for semantic search
- Retrieved using **cosine similarity**

---

### 3ï¸âƒ£ **Long-term Memory (Structured Facts)**
- **Storage**: PostgreSQL table `structured_facts` WITHOUT embeddings
- **Capacity**: Small (~10-50 facts per user)
- **Lifecycle**: Persistent with versioning
- **Purpose**: Critical user facts with guaranteed exact recall

```sql
-- Database schema
CREATE TABLE structured_facts (
    id              SERIAL PRIMARY KEY,
    user_id         TEXT NOT NULL,
    category        TEXT NOT NULL,   -- identity|preference|constraint|instruction
    key             TEXT NOT NULL,   -- name|language|timezone|coding_style
    value           TEXT NOT NULL,
    confidence      FLOAT DEFAULT 1.0,
    importance      FLOAT DEFAULT 0.8,
    is_active       BOOLEAN DEFAULT TRUE,
    UNIQUE (user_id, category, key, is_active)  -- Only one active value per key
);
```

**Key Properties:**
- **NO embeddings** (pure deterministic lookup)
- **Conflict resolution** via UNIQUE constraint
- Only stores **structured facts**, not conversation

---

## ğŸ”„ Flow 1: How Conversations Move to Mid-term Memory

### Trigger: Every 20 Turns

```
Turn 1-20:  [user msg] â†’ Add to session â†’ [assistant response] â†’ Add to session
            â†“ (all in RAM)
Turn 21:    Session.length = 20 â†’ TRIGGER COMPRESSION
```

### Compression Process (Automatic)

```python
# In app.py, line 71-81:

if should_summarize(len(session.history)):  # Check if >= 20 turns
    # 1. Take OLDEST turns (keep last 10 in session)
    to_compress = session.history[:-10]  # e.g., turns 1-10
    
    # 2. Submit async job (doesn't block response)
    _executor.submit(
        compress_session_to_episodic,
        user_id,
        to_compress,
        current_turn - len(session.history)
    )
    
    # 3. Remove compressed turns from session
    session.history = session.history[-10:]  # Keep only last 10
```

### What Happens in `compress_session_to_episodic()`:

```
Step 1: Summarize
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input: [turn1, turn2, ..., turn10]
       â†“ (send to LLM)
Output: "User asked about vector databases. Assistant explained FAISS vs pgvector..."

Step 2: Generate Embedding
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Summary text â†’ SentenceTransformer â†’ 384-dim vector

Step 3: Store in Database
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
INSERT INTO episodic_memory (user_id, summary, embedding, turn_start, turn_end)
VALUES ('user123', 'User asked about...', [0.12, -0.34, ...], 1, 10)
```

### Result:
- âœ… 10 turns compressed into 1 summary
- âœ… Summary has embedding for semantic search
- âœ… Short-term memory freed up
- âœ… Context preserved for future retrieval

---

## ğŸ”„ Flow 2: How Long-term Facts Are Maintained

### Extraction Trigger: Every User Message

```python
# In app.py, line 118:
# After LLM response, extract facts in background
_executor.submit(extract_and_store, user_id, message)
```

### Extraction Process

```
Step 1: Detect Facts
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User: "My name is Alex and I prefer Python"
       â†“ (send to LLM with extraction prompt)
LLM Output: [
    {"category": "identity", "key": "name", "value": "Alex", "confidence": 1.0},
    {"category": "preference", "key": "language", "value": "Python", "confidence": 0.9}
]

Step 2: Validate & Filter
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
For each extraction:
- Check confidence >= 0.4
- Check importance >= 0.2
- Validate category is valid (identity/preference/constraint/instruction)
- Deduplicate within message

Step 3: Store (with Conflict Resolution)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
INSERT INTO structured_facts (user_id, category, key, value, confidence)
VALUES ('user123', 'identity', 'name', 'Alex', 1.0)
ON CONFLICT (user_id, category, key, is_active)
DO UPDATE SET 
    value = EXCLUDED.value,
    confidence = EXCLUDED.confidence,
    updated_at = NOW()
WHERE structured_facts.confidence <= EXCLUDED.confidence  â† Only update if new confidence is higher!
```

### Conflict Resolution Example:

```
Turn 5:  "My name is Alex"  â†’ confidence=1.0 â†’ STORED
Turn 50: "Call me Al"       â†’ confidence=0.6 â†’ IGNORED (lower confidence)
Turn 80: "My full name is Alexander" â†’ confidence=0.95 â†’ UPDATED
```

### What Gets Stored:
- âœ… Identity: name, pronouns, location
- âœ… Preferences: language, timezone, coding style
- âœ… Constraints: dietary restrictions, availability
- âœ… Instructions: standing rules, workflows

### What Doesn't Get Stored:
- âŒ Conversational details ("we discussed X")
- âŒ Temporary context ("I'm currently working on Y")
- âŒ Low-confidence extractions
- âŒ Low-importance information

---

## ğŸ” Flow 3: How Retrieval Works (3-Stage Pipeline)

### Trigger: Every Query (if mode="active")

```python
# In app.py, line 88:
retrieval_results = retrieve_all(user_id, message, top_k_episodes=3)
```

### Stage 1: Deterministic Facts Lookup (GUARANTEED RECALL)

```sql
-- In structured_facts.py
SELECT category, key, value, confidence, importance
FROM structured_facts
WHERE user_id = 'user123'
  AND is_active = TRUE        â† Only active facts
  AND importance >= 0.5       â† High importance only
ORDER BY importance DESC;     â† Most important first
```

**Why Deterministic?**
- No vectors involved
- Exact match by SQL
- **Zero false positives**
- **Guaranteed to find** if it exists

**Example Result:**
```python
[
    {"category": "identity", "key": "name", "value": "Alex", "confidence": 1.0},
    {"category": "preference", "key": "language", "value": "Python", "confidence": 0.9},
    {"category": "preference", "key": "coding_style", "value": "black", "confidence": 0.85}
]
```

---

### Stage 2: Episodic Vector Search (SEMANTIC CONTEXT)

```sql
-- In episodic_store.py
-- 1. Embed the query
query_embedding = embed("What did we discuss about databases?")  -- [0.23, -0.45, ...]

-- 2. Vector similarity search
SELECT turn_range, summary, turn_start, turn_end,
       embedding <=> query_embedding AS distance  â† Cosine distance operator
FROM episodic_memory
WHERE user_id = 'user123'
  AND embedding <=> query_embedding < 0.4  â† Distance threshold
ORDER BY distance ASC  â† Most similar first
LIMIT 3;
```

**Why Vector Search?**
- Finds **semantically related** conversations
- Works even if exact words don't match
- Example: "databases" query matches episode about "FAISS and pgvector"

**Example Result:**
```python
[
    {
        "turn_range": "turns 1-10",
        "summary": "User asked about vector databases. Discussed FAISS vs pgvector...",
        "distance": 0.12
    },
    {
        "turn_range": "turns 21-30", 
        "summary": "Conversation about retrieval systems and embeddings...",
        "distance": 0.28
    }
]
```

---

### Stage 3: Ranking & Formatting (IMPLICIT)

```python
# In retrieval_harness.py

# Facts are already ordered by importance (from SQL ORDER BY)
# Episodes are already ordered by distance (from SQL ORDER BY)

# Format for prompt injection:
def format_for_injection(results):
    output = []
    
    # Section 1: Structured Facts
    output.append("## User Profile")
    for fact in results["structured_facts"]:
        output.append(f"- {fact['key']}: {fact['value']}")
    
    # Section 2: Relevant Episodes
    output.append("\n## Recent Context")
    for episode in results["episodic_context"]:
        output.append(f"- {episode['turn_range']}: {episode['summary'][:150]}...")
    
    return "\n".join(output)
```

**Final Prompt Injection:**
```
# Long-term Memory
## User Profile
- name: Alex
- language: Python
- coding_style: black with line length 100

## Recent Context
- turns 1-10: User asked about vector databases. Discussed FAISS vs pgvector...
- turns 21-30: Conversation about retrieval systems and embeddings...

# Recent Conversation
user: What did we discuss about databases?
assistant: ...

Answer consistently using the memory and conversation above.
```

---

## ğŸ“ˆ Complete Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  USER MESSAGE                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Add to Session       â”‚ â† Short-term (RAM)
         â”‚  (last 10-20 turns)   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
         â•‘ If session >= 20 turnsâ•‘
         â•šâ•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•
                     â†“ YES
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  COMPRESS TO EPISODIC â”‚
         â”‚  - Summarize oldest   â”‚
         â”‚  - Generate embedding â”‚ â† Mid-term (PostgreSQL)
         â”‚  - Store in DB        â”‚
         â”‚  - Clear from session â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  3-STAGE RETRIEVAL    â”‚
         â”‚                       â”‚
         â”‚  Stage 1:             â”‚
         â”‚  â””â”€ Deterministic     â”‚ â† Long-term (PostgreSQL)
         â”‚     fact lookup       â”‚
         â”‚                       â”‚
         â”‚  Stage 2:             â”‚
         â”‚  â””â”€ Episodic vector   â”‚ â† Mid-term (PostgreSQL)
         â”‚     search            â”‚
         â”‚                       â”‚
         â”‚  Stage 3:             â”‚
         â”‚  â””â”€ Format & rank     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  BUILD PROMPT         â”‚
         â”‚  - Memory injection   â”‚
         â”‚  - Short-term context â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  LLM CALL             â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  RESPONSE             â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  EXTRACT FACTS        â”‚
         â”‚  (async background)   â”‚
         â”‚  - Parse with LLM     â”‚ â† Long-term (PostgreSQL)
         â”‚  - Validate           â”‚
         â”‚  - Store/Update       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Key Innovations

### 1. Automatic Transition Between Tiers
- Short â†’ Mid: Happens at 20 turns (configurable)
- User â†’ Long: Happens after every message (async)
- **No manual intervention needed**

### 2. Hybrid Retrieval
- **Deterministic** for critical facts (name, preferences)
- **Semantic** for conversational context
- Best of both worlds!

### 3. Memory Efficiency
- 100 turns = ~15 facts + 5 episodes (not 100 records!)
- Compression ratio: ~20:1

### 4. Conflict Resolution
- Only updates with higher confidence
- Prevents degradation of memory quality
- Maintains history (via updated_at)

### 5. Async Processing
- Extraction doesn't block response
- Compression doesn't block response
- User gets <500ms latency

---

## ğŸ’¡ Example: 100-Turn Conversation

```
Turn 1-20:   All in session (RAM)
             â†’ At turn 21: Compress turns 1-10 â†’ episodic_memory[0]
             â†’ Facts extracted: name, language, timezone â†’ structured_facts

Turn 21-40:  Last 10 in session + turns 11-20 in session
             â†’ At turn 41: Compress turns 11-30 â†’ episodic_memory[1]
             â†’ More facts extracted: coding_style â†’ structured_facts

Turn 41-60:  Last 10 in session + turns 31-40 in session
             â†’ At turn 61: Compress turns 31-50 â†’ episodic_memory[2]
             
... continues ...

Turn 100:    Session has turns 91-100
             episodic_memory has 4 summaries (turns 1-80)
             structured_facts has ~15 key facts
             
Query: "What's my name?"
  â†’ Stage 1: SQL lookup â†’ "Alex" (guaranteed)
  
Query: "What did we discuss about databases?"
  â†’ Stage 2: Vector search â†’ episodic_memory[0] (turns 1-10)
```

---

This architecture scales to **1000+ turns** because:
- âœ… Short-term stays bounded (10-20 turns)
- âœ… Long-term stays small (only structured facts)
- âœ… Mid-term grows linearly but efficiently (compressed summaries)
- âœ… Retrieval is fast (indexed SQL + vector search)
